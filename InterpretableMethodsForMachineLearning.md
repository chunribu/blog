[🏠HOME](./README.md)

# 机器学习可解释性方法（Interpretable methods for machine learning）

---

> Source: [Chapter 5 Model-Agnostic Methods](https://christophm.github.io/interpretable-ml-book/agnostic.html) by [Christoph Molnar](https://github.com/christophM/)

无关模型的方法（Model-Agnostic Methods, MAMs）将解释与机器学习模型分离，它存在一些优势(Ribeiro et al. 2016)，最主要的是灵活性。当解释方法可以应用于任何模型时，机器学习开发者可以自由使用他们喜欢的任何机器学习模型。 任何建立在机器学习模型解释上的东西，如图形或用户界面，也变得独立于底层的机器学习模型。通常，需要评估多个类型的机器学习模型来解决一个任务，而非一种，当在可解释性方面比较模型时，因为同样的方法可以用于任何类型的模型，使用MAMs更容易。

使用可解释的模型可以不需要MAMs，但这样有一个很大的缺点，即与其他机器学习模型相比，预测性能会丧失，而且你可能会把自己限制在一种类型的模型上。还有一个替代方法是使用特定于模型的解释方法，它的缺点同样是会把你束缚在一种模型类型上。

一个与模型无关的解释系统的理想方面是（Ribeiro et al, 2016）。

- **模型的灵活性：** 解释方法可以适用于任何机器学习模型，如随机森林和深度神经网络。 
- **解释的灵活性：** 不局限于某种形式的解释。在某些情况下，有一个线性公式可能是有用的，在其他情况下，有特征导入的图形。 
- **表示的灵活性：** 解释系统应该能够使用不同的特征表示作为被解释的模型。对于一个使用抽象词嵌入向量的文本分类器，可能最好使用单个单词的存在来解释。

## The big picture

让我们从整体上来看与模型无关的可解释性。我们通过观测世界来收集数据，用机器学习模型来预测数据，以进一步抽象它。可解释性只是最顶上的一层，用来帮助人类理解。

![pic](src/big-picture.png)

最底层是**世界**。从字面上看，这可能是自然界本身，如人体的生物学以及它对药物的反应，但也有更抽象的东西，如房地产市场。世界层包含了所有可以观察到的、有兴趣的东西。最终，我们想了解一些关于世界的东西，并与之交互。

第二层是**数据**层。我们必须将世界数字化，以使其可被计算机处理，并存储信息。数据层包含从图像、文本、表格数据等任何东西。通过在数据层的基础上拟合机器学习模型，我们得到了**黑盒模型**层。机器学习算法通过现实世界的数据进行学习，以进行预测或寻找结构。在黑盒模型层之上是**可解释性方法**层，它帮助我们处理机器学习模型的不透明性。对于一个特定的诊断，最重要的特征是什么？为什么一个金融交易被归类为欺诈？

最后一层是由一个**人**占据的。看! 这个人向你招手，因为你在读这本书，帮助为黑盒模型提供更好的解释！人类最终是解释的消费者。

这种多层次的抽象也有助于理解统计学家和机器学习从业者之间的方法差异。统计学家处理的是数据层，如规划临床试验或设计调查。他们跳过黑盒模型层，直接进入可解释性方法层。 机器学习专家也处理数据层，比如收集皮肤癌图像的标记样本或爬行维基百科。然后他们训练一个黑盒机器学习模型。可解释性方法层被跳过，人类直接处理黑盒模型的预测。可解释性机器学习融合了统计学家和机器学习专家的工作，这很好。

当然，这个图形并没有捕捉到所有的东西：数据可能来自于模拟。黑盒模型也会输出预测，这些预测甚至可能没有达到人类，而只提供给其他机器。但总的来说，这是一个有用的抽象，可以理解可解释性如何成为机器学习模型之上的这个新层。

## References
+ Ribeiro, Marco Tulio, Sameer Singh, and Carlos Guestrin. "Model-agnostic interpretability of machine learning." ICML Workshop on Human Interpretability in Machine Learning. (2016).
